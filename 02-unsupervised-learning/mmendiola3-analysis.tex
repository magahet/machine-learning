\documentclass{sig-alternate}
\usepackage{etoolbox}
\makeatletter
\patchcmd{\maketitle}{\@copyrightspace}{}{}{}
\makeatother

\usepackage[caption=false]{subfig}
\usepackage{verbatimbox}
\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}




\begin{document}

\title{Randomized Optimization, Unsupervised Learning, and Dimensionality Reduction}
\subtitle{Writeup for Assignment 02 - CS 6741}

\author{
\alignauthor
Magahet Mendiola
}
\date{}

\maketitle
\begin{abstract}
An empirical analysis of randomized optimization, clustering, and dimensionality reduction algorithms.
\end{abstract}

\section{Part 1: Randomized Optimization}

In order to compare and contrast the given randomized optimization algorithms, a classification task was taken from the previous assignment and each algorithm was utilized to train a neural network to perform this task. Implementation of each algorithm as well as the code for running the neural network was taken from the ABAGAIL machine learning library (https://github.com/pushkar/ABAGAIL).

\subsection{Neural Network Training}

\paragraph{Classification Task}

The dataset used to test our neural network optimization task was the Adult dataset from the UCI repository. This was the second of the two classification datasets used in the supervised learning assignment. To recap, these are samples of personal socioeconomic attributes with a binary classification of gross net income over or below 50k/yr.

This dataset should prove interesting as an optimization task, since we have a fairly clear intuition of the relationship between the attributes and the classifications. From the previous assignment, we could see that attributes such as capital gain or loss had a strong linear relationship to the classification, as did education level. Intuition, or in this context domain knowledge, suggests that the fitness function for our neural network will have a smooth surface with predictable gradients. This is due to belief that having a slightly higher or lower education level would have a slightly higher or lower (mostly predictable) impact on one's net worth. In the case of our neural network, the actual fitness function is the sum of squared errors between the network's output and the actual instance binary values.

The optimization task was run against a training set of 1,000 instances from the adult dataset and final classification error was then measured with a separate testing set of another 1,000 instances. 19 attributes were used from the set and nominal attributes were first converted into separate binary attributes.

\paragraph{Optimization Parameters}

Randomized hill climbing, simulated annealing, a genetic algorithm, and gradient decent were all utilized to train the weights for our neural network. The network consisted of 19 input nodes, 5 hidden nodes, and 1 output node. Each algorithm was given 1,000 training iterations to find an optimum set of weights. Simulated annealing was set with an initial temperature of 1\e{11} and a cooling exponent of 0.95. The genetic algorithm used a population of 200 with a set of 100 used for crossover and another 10 used for mutation.

\paragraph{Optimization Results}

Using three randomized optimization algorithms, our neural network was trained over 1000 iterations each. The classification accuracy of each of these trained networks is shown in Figure~\ref{ann-error-summary}. As shown, randomized hill climbing outperformed the rest both in terms of training the network to correctly classify instances and in the time required to train the network. Simulated Annealing was negligibly faster, but the resulting network did not perform as well in the classification task.

\begin{verbbox}
Results for RHC: 
Correctly classified 764.0 instances.
Incorrectly classified 236.0 instances.
Percent correctly classified: 76.400%
Training time: 3.709 seconds
Testing time: 0.004 seconds

Results for SA: 
Correctly classified 733.0 instances.
Incorrectly classified 267.0 instances.
Percent correctly classified: 73.300%
Training time: 3.023 seconds
Testing time: 0.004 seconds

Results for GA: 
Correctly classified 763.0 instances.
Incorrectly classified 237.0 instances.
Percent correctly classified: 76.300%
Training time: 72.068 seconds
Testing time: 0.004 seconds

Results for GD: 
Correctly classified 725.0 instances.
Incorrectly classified 175.0 instances.
Percent correctly classified: 72.500%
Training time: 6.560 seconds
Testing time: 0.004 seconds
\end{verbbox}

\begin{figure}[!htbp]
    \centering
    \theverbbox
    \caption{Randomized optimization training on ANN results\label{ann-error-summary}}
\end{figure}

This result reinforces our intuition regarding the dataset. Randomized hill climbing is similar to gradient decent in that the algorithm makes small changes in a direction that improves the fitness function result. Since we believe the profile of the adult dataset to be smooth and behaves predictably given changes in the inputs, both gradient decent and randomized hill climbing should move quickly toward a local minimum. In this case, since simulated annealing and the genetic algorithm did not find a solution that performed better than these two, we make the claim that this local minimum is in fact the global minimum.

\paragraph{Training Performance}

It is interesting to note the performance of each algorithm throughout the set of training cycles. This gives a better picture of how each optimization process interacted with the fitness function. In the case of randomized hill climbing (Figure~\ref{error-rhc}) the learning curve appears to smoothly converge toward lower classification error rates. This shows that the algorithm works well in this case in moving the fitness function step by step towards an optimum.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=3in]{part1.1/error-rhc.pdf}
    \caption{randomized hill climbing ANN classification error by training iterations \label{error-rhc}}
\end{figure} 

As the error rate does not seem to stabilize over the 1,000 iterations, an additional experiment was performed to see if it could continue to improve the weights if allowed to run for 10,000 iterations. The results are shown in Figure~\ref{error-rhc-10x}. You can see that the algorithm continues to find weights that result in a lower classification error on the training set. However, the resulting network resulted in almost the same classification error when run against the set aside testing set. This result illustrates the previous understanding of neural network performance and it's ability to overfit the training data.


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=3in]{part1.1/error-rhc-10x.pdf}
    \caption{randomized hill climbing ANN classification error by training iterations \label{error-rhc-10x}}
\end{figure} 

Figure~\ref{error-sa} shows the results of simulated annealing over the 1,000 training iterations. We see that, unlike randomized hill climbing, the error increases for more than a single iteration, and increases more towards the beginning of the process. When the algorithm fist starts, temperature is still high enough to allow movement toward less optimal results. Around 300 iterations in we see a dramatic improvement in the error. As the temperature cools, the variation in error decreases and the algorithm converges toward local optimum. In this particular optimization problem, the result of this movement in sub-optimal directions most likely caused a delay in reaching an optimal point. However, given a fitness function with a number of local optima, this behavior would have helped to discover remote valleys.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=3in]{part1.1/error-sa.pdf}
    \caption{simulated annealing ANN classification error by training iterations \label{error-sa}}
\end{figure} 

Our genetic algorithm converged very quickly toward an optimum set of weights. However, elapsed time was an order of magnitude greater than any of the other optimization algorithms. This is likely due to the computation involved in maintaining the population and performing crossover and mutation functions. This can be somewhat offset by the fact that the genetic algorithm reached a optimum set of weights in roughly half the number of iterations, but that still puts elapsed time near five times that of the others.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=3in]{part1.1/error-ga.pdf}
    \caption{genetic algorithm ANN classification error by training iterations \label{error-ga}}
\end{figure} 


randomized hill climbing
simulated annealing
a genetic algorithm
MIMIC

%You will then use the first three algorithms to find good weights for a neural network. In particular, you will use them instead of backprop for the neural network you used in assignment #1 on at least one of the problems you created for assignment #1. Notice that weights in a neural network are continuous and real-valued instead of discrete so you might want to think a little bit about what it means to apply these sorts of algorithms in such a domain.

\subsection{Fitness Functions}

In order to examine the properties of each randomized optimization algorithm, a number of experiments were conducted on contrived fitness functions. Each of these functions accepts an arbitrary length bit string and returns a single positive value. In order to visualize the features of these functions, they have each been plotted with the fitness score on the y axis and the input bit string encoded as a integer value on the x axis. Although this only presents a two dimensional projection of the function, it does provide some sense of behavior, including a view of local and global optima. 

For each fitness function, a series of experiments were run against each optimization algorithm. First, each algorithm was allowed to train against a fixed input space size of 80 bits. The algorithms were allowed to perform a maximum of 10,000 function evaluations before being terminated. This experiment was repeated 10 times for each algorithm and plotted to allow us to visualize the behavior of each optimization algorithm on the given fitness function as they navigated the output terrain.

The second experiment was setup to evaluate how many fitness function evaluations each algorithm required to find one of the global optima. Each algorithm was allowed to run indefinitely, until it either became stuck at a local optimum or found one of the global optima. In the former case, the algorithm was reset and re-run. This experiment was repeated for differing input space sizes (bit string lengths). The results from this set of experiments was to see how well each algorithm performed at finding the function optimum. It also provided an understanding of how well each algorithm performed given differing input sizes.

\subsubsection{Optimization Algorithm Parameters}

Each algorithm was given the same set of parameters for each experiment. 

Random hill climbing was setup to explore an input change of a single random bit and move in that direction if the fitness score improved. If an improved input was not found in a set number of evaluations (the number of input bits), the algorithm was restart with a new randomized input.

The temperature parameter for our simulated annealing algorithm was set at 1,000. This temperature was cooled by reducing it by 5\% each iteration. 

Our genetic algorithm maintained a population of 100. 50 of those were used to perform uniform bit crossover. The other 50 were mutated by swapping two random bits in their sequence.

The MIMIC algorithm was set to maintain a population of 100.


\subsubsection{Count Ones}

The count ones fitness function is well understood and easy to explain. The function simply counts the number of 1s in the input bit string. Although this is an easy heuristic to follow when calculating the function return value, the function's behavior over the input space was interesting to visualize. Figure~\ref{plot-count-ones} shows the plot of the count ones function. As we see, there are many local optima and a single global optimum. The function has a upward trend with a stair step like behavior along the way.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=3in]{part1.2/count-ones-plot.pdf}
    \caption{fitness function plot - count-ones\label{plot-count-ones}}
\end{figure} 

Figure~\ref{over-time-count-ones} shows how our optimization algorithms performed against the count ones function with an 80 bit input. Random hill climbing appeared to discover peaks quickly. However, the global optimum was not reached in many cases until twice the number of evaluations as simulated annealing. RHC also suffered from it's random restart behavior, which caused many function evaluations at much lower values of the function.

Simulated annealing was clearly the superior algorithm in this experiment. All 10 runs reached the global optimum within roughly 1,000 function evaluations, which represents 1/1\e{21} of the total input space. The genetic algorithm and MIMIC behaved well, making progress with each evaluation. However, they both required an order of magnitude greater evaluations to move toward the global optimum.

Given the relatively smooth trend of the function toward a global optimum, simulated annealing seems to have been able to explore the space well enough to move in that direction. As we will see later, finding a suitable temperature parameter is crucial for allowing the algorithm to move out of local optima. However, high temperatures keep the optimization task geared towards exploration longer than is necessary. In the case of this experiment, we happen to stumble on a good balance between exploration and exploitation.


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=3in]{part1.2/count-ones-over-time.pdf}
    \caption{fitness score by iteration - count-ones\label{over-time-count-ones}}
\end{figure} 

Each algorithm performed reasonably well at finding the global optimum for the count ones function at various input sizes, with the exception of our genetic algorithm. Given the somewhat unstructured and independent nature of the input bits, this is not surprising. MIMIC seemed to do well in this regard, despite the lack of structure. We could understand this to be a fortunate side effect of the probabilistic sampling, which causes the algorithm to perform similar to simulated annealing. Figure~\ref{max-count-ones} shows this behavior.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=3in]{part1.2/count-ones-max.pdf}
    \caption{iterations to find max - count-ones\label{max-count-ones}}
\end{figure} 


\subsubsection{Four Peaks}

The four peaks evaluation function is essentially a count of the leading 1s and trailing 0s in a bit string, with a bonus for having at least an arbitrary number of each. The plot of this function shows there are two global optima and a large number of local optima. There is also a point along the plot where values shift upwards. This is the point when the leading 1s reach the minimum number to get the bonus.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=3in]{part1.2/four-peaks-plot.pdf}
    \caption{fitness function plot - four-peaks\label{plot-four-peaks}}
\end{figure} 

Our genetic algorithm was able to find reasonably high valued input strings with a small number of function evaluations (Figure~\ref{over-time-four-peaks}. This makes sense given the structure of the input space and the fact that the algorithm would quickly evolve the population to include bit strings with better features (longer sets of leading 1s and trailing 0s). However, this algorithm did seem to struggle to find one of the two global optima.

This function also highlights the shortcomings of simulated annealing. Our plot shows that 7 out of 10 runs became permanently stuck in a local optimum. This is likely due to the sharp features of the landscape and insufficiently low temperature and overly rapid cooling rate. This caused the algorithm to get caught on a lower range of peaks.


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=3in]{part1.2/four-peaks-over-time.pdf}
    \caption{fitness score by iteration - four-peaks\label{over-time-four-peaks}}
\end{figure} 

Figure\ref{max-four-peaks} shows that although our genetic algorithm was able to quickly find input strings with high values, it required many more iterations to find the global optimum for a given input size. Conversely, despite the impressive figures for simulated annealing, we noted in the previous experiment how that algorithm was too likely to get caught in a local optimum. It required many more iterations of this experiment to get simulated annealing to find the global optimum at all.

MIMIC seemed to behave well in this case. Although it did not locate optimal values as quickly as the other algorithms, it showed steadily improving results over time and did not have the same difficulty with local optima. Also, in terms of iterations to finding the global optimum, MIMIC performed significantly better than the genetic algorithm for higher input sizes.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=3in]{part1.2/four-peaks-max.pdf}
    \caption{iterations to find max - four-peaks\label{max-four-peaks}}
\end{figure} 

\subsubsection{Flip Flop}

The flip flop fitness function is scored by adding one for each bit that has a different value than the next bit. There are two global optima for any given bit length and the function terrain is symmetrical over the input space, as we can see in Figure~\ref{plot-flip-flop}. This function combines features of both count ones and four peaks. It acts as a semi-structured function as each input bit has a relationship to the bit's on either side. However, there are two distinct optima with a completely opposite bit pattern.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=3in]{part1.2/flip-flop-plot.pdf}
    \caption{fitness function plot - flip-flop\label{plot-flip-flop}}
\end{figure} 

Each of our three algorithms, not including random hill climbing, do well over time on this function. Simulated annealing is able to converge to high values of the function with significantly less evaluations than the other two. The genetic algorithm and MIMIC both move steadily toward higher values, but with many more evaluations. Mimic is able to do so with more consistent improvement and less variance in score along the way.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=3in]{part1.2/flip-flop-over-time.pdf}
    \caption{fitness score by iteration - flip-flop\label{over-time-flip-flop}}
\end{figure} 

Each algorithm was able to find the one of the optima within a closer number of evaluations than the previous fitness functions (Figure~\ref{max-flip-flop}. However, simulated annealing still performed orders of magnitude better than both MIMIC and the genetic algorithm in this experiment. 

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=3in]{part1.2/flip-flop-max.pdf}
    \caption{iterations to find max - flip-flop\label{max-flip-flop}}
\end{figure} 

%In addition to finding weights for a neural network, you must create three optimization problem domains on your own. For the purpose of this assignment an optimization problem is just a fitness function one is trying to maximize (as opposed to a cost function one is trying to minimize). This doesn't make things easier or harder, but picking one over the other makes things easier for us to grade.

%Please note that the problems you create should be over discrete-valued parameter spaces. Bit strings are preferable.

%The first problem should highlight advantages of your genetic algorithm, the second of simulated annealing, and the third of MIMIC. Be creative and thoughtful. It is not required that the problems be complicated or painful. They can be simple. For example, the 4-peaks and k-color problems are rather straightforward, but illustrate relative strengths rather neatly.


\section{Part 2: Unsupervised Learning Algorithms}

%You are to run a number of experiments. Come up with at least two datasets. If you'd like (and it makes a lot of sense in this case) you can use the ones you used in the first assignment or, if it makes any sense, the first part of this assignment.

\subsection{Exploring Dataset 1: Adult}

The first dataset we will explore is the same as from Part 1, the Adult dataset from UCI. To begin, we should first examine the distribution of instances by their class label. As we see in Figure~\ref{adult-attr-class} and Figure~\ref{adult-parallel-class}, groupings by class seem to occur based on a attributes such as workclass, capital-gain, hour-per-week, and education. However, these groupings are not completely obvious and there are many non-conforming samples. As we will see with the results from our clustering task, the two class labels do not segment the data as well it could be done.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=3in]{part2/adult/attr-class.pdf}
    \caption{dataset attributes - adult\label{adult-attr-class}}
\end{figure} 

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=3in]{part2/adult/parallel-class.pdf}
    \caption{dataset parallel plot - adult\label{adult-parallel-class}}
\end{figure} 

\subsubsection{Clustering}

%Run the clustering algorithms on the data sets and describe what you see.

In order to determine the optimal value for K, we can iterate through increasing values and compare some metric that gives an idea of how well our data is segmented into each cluster. In our case, the expectation maximization algorithm was run with increasing values of K, and the resulting log likelihood value was compared. This is a default option in the Weka implementation of the algorithm. Larger and smaller values of K were run to validate the result through visual inspection of the resulting cluster features. For the Adult dataset, a cluster size of three resulted in the greatest log likelihood, and the clusters made sense when viewing the resulting cluster attributes.

\tiny
\begin{verbbox}
Running k-means

Within cluster sum of squared errors: 94691.74506148841
Missing values globally replaced with mean/mode

Cluster centroids:
                                                              Cluster#
Attribute                                Full Data                   0                   1                   2
                                           (32561)              (7243)             (14601)             (10717)
==============================================================================================================
age                                        38.5816             43.6339             32.9471             42.8437
workclass                                  Private             Private             Private             Private
fnlwgt                                 189778.3665         187061.4272         192070.6165         188491.5931
education                                  HS-grad           Bachelors             HS-grad             HS-grad
education-num                              10.0807             12.6281              9.8104              8.7273
marital-status                  Married-civ-spouse  Married-civ-spouse       Never-married  Married-civ-spouse
occupation                          Prof-specialty      Prof-specialty        Adm-clerical        Craft-repair
relationship                               Husband             Husband       Not-in-family             Husband
race                                         White               White               White               White
sex                                           Male                Male              Female                Male
capital-gain                             1077.6488           3607.9214            269.1136            469.1444
capital-loss                               87.3038            179.5659             50.7723             74.7204
hours-per-week                             40.4375             45.0614             36.5254             42.6422
native-country                       United-States       United-States       United-States       United-States
class                                        <=50K                >50K               <=50K               <=50K
\end{verbbox}
\normalsize


\begin{figure}[!htbp]
    \centering
    \theverbbox
    \caption{Randomized optimization training on ANN results\label{kmeans-summary}}
\end{figure}


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=3in]{part2/adult/attr-cluster.pdf}
    \caption{dataset attributes - adult\label{adult-attr-cluster}}
\end{figure} 

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=3in]{part2/adult/parallel-cluster.pdf}
    \caption{dataset parallel plot - adult\label{adult-parallel-cluster}}
\end{figure} 


\subsubsection{Dimensionality Reduction}

%Apply the dimensionality reduction algorithms to the two datasets and describe what you see.

\subsubsection{Clustering on Dimensionally Reduced Dataset}

%Reproduce your clustering experiments, but on the data after you've run dimensionality reduction on it.

\subsection{Exploring Dataset 2: TODO}

\subsubsection{Clustering}

%Run the clustering algorithms on the data sets and describe what you see.

\subsubsection{Dimensionality Reduction}

%Apply the dimensionality reduction algorithms to the two datasets and describe what you see.

\subsubsection{Clustering on Dimensionally Reduced Dataset}

%Reproduce your clustering experiments, but on the data after you've run dimensionality reduction on it.


\subsection{Neural Network Training}


\subsubsection{Training on Dimensionally Reduced Dataset}
%Apply the dimensionality reduction algorithms to one of your datasets from assignment #1 (if you've reused the datasets from assignment #1 to do experiments 1-3 above then you've already done this) and rerun your neural network learner on the newly projected data.


\subsubsection{Training on Dimensionally Reduced Dataset Clusters}

%Apply the clustering algorithms to the same dataset to which you just applied the dimensionality reduction algorithms (you've probably already done this), treating the clusters as if they were new features. In other words, treat the clustering algorithms as if they were dimensionality reduction algorithms. Again, rerun your neural network learner on the newly projected data.






\begin{thebibliography}{10}

\bibitem{Bache+Lichman:2013}
K. Bache and M. Lichman
\newblock UCI Machine Learning Repository
\newblock 2013
\newblock http://archive.ics.uci.edu/ml
\newblock University of California, Irvine, School of Information and Computer Sciences

%\bibitem{Zhong:Osteo}
%L.~Zhong, D.~El-Daye, B.~Kaufman, N.~Tobaoda, T.~Mohamed, and M.~Liebschner.
%\newblock Osteoconduct: Wireless body-area communication based on bone
  %conduction.
%\newblock In {\em in Proc. Int. Conf. Body Area Networks (BodyNets)}, June
  %2007.

\end{thebibliography}
\end{document}
